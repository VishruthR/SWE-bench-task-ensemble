# SWE-Bench w/ Ensemble Agents

## Overview

SWE-bench is a benchmark that measures an AI agents ability to solve real-world software engineering tasks by submitting patches for issues on open-source repositories. Many agents have attempted the various SWE-bench tasks and the best agents complete about 64% of issues on the [SWE-bench verified](https://www.swebench.com/#verified) dataset. We want to test if using an ensemble method, selecting the best patch from multiple agents, can improve performance on SWE-bench. 

Research question: If we take the best output for each AI agent on each task, how much does our performance on SWE-bench improve?

## Methodology

SWE-bench works by giving an AI agent a repository and an issue text description. The AI agent writes a patch that is then applied to the codebase. Finally, the test cases within the codebase (along with new test cases written by human developers) are run to ensure the patch passes all unit tests. We propose that selecting the best patch out of a group of patch generations from various AI agents will outperform the performance of a single AI agent.

We must find a way to identify the best patch for each task. To do this, we will use the test case generation method proposed by [UTBoost](https://openreview.net/forum?id=I4tNPWHM6e). For each task, we will generate test cases to verify if the issue was resolved. It is important to note that within SWE-bench, each task has one or more unit tests added to it to verify that the issue has been fixed correctly (these are unit tests usually written by a human developer when the issue was resolved in the real repository). To ensure we are not selecting the best patch based on the unit tests added after the issue has been resolved, we replace those unit tests with the ones we generate using UTBoost. This means we only select agents based on the existing test cases in the repository before the issue was resolved and the new test cases we generated, mimicking the information that would be available to a developer in the real-world (our generated test cases are analogous to the unit tests a developer might come up with in the course of resolving a task).

The best patch for each task is selected based on pass rate on test cases for that task, with tiebreakers resolved arbitrarily.

## Implementation and results

The UTBoost paper mentions that it costs about $1.60 USD to generate test cases for a single task. Fortunately, the [UTBoost](https://anonymous.4open.science/r/UTBoost-7224/readme.md) repository has generated test cases for us to use. Unfortunately, these test cases are generated using and old version of GPT 4o, which is no longer SOTA. Another concern is that many tasks had no additional test cases generated (presumably because UTBoost determined that no additional test cases were needed). Regardless, these tests will serve as a good baseline to verify if this method has promise for future research.

To save on costs we perform the initial tests on a subset of 30 randomly-selected tasks from SWE bench Verified. Testing the top 20 agents on this subset of tasks, the best results are from agentscope which resolved 27/30 (90%) tasks. The worst performance was by enginelabs which resolved 18/30 (60%) tasks. Notably, it seems the best agents perform significantly better on this subset of tasks than the full swe-bench verified dataset where the best agents resolved around 65% of the tasks. This subset of tasks includes 15 tasks from django and 4 from matplotlib, two of the repositories agents tend to perform better on than other repositories. For our purposes, we care about the improvement in performance, so this is not a major concern.

The ensemble method resolved 25/30 (83%) of tasks. While theses results initially seem to disprove our hypothesis, there is some evidence that the ensemble method can improve performance. Many of the tasks had no additional test cases generated by UTBoost and some of the generated test cases were not sufficient to catch potential bugs. This meant most patches achieved the same pass rate on a given task and therefore the best patch for that task was chosen by an arbitrary tiebreaker. Because of this, 28/30 tasks used the patch from augment_agent_v0 (augment_agent_v0 won our arbitrary tiebreaker). augment_agent_v0 resolved 24/30 (80%) of tasks without the ensemble method, indicating that the ensemble method could select patches to improve upon a single agent's performance. Furthermore, the ensemble method resolved one task which agentscope (the best performing individual agent) failed to resolve, indicating potential for improvement. 

While the initial tests did not prove that ensemble patch generation using UTBoost for agent selection can improve overall performance on SWE-related tasks, there were promising results that indicate the need for further research. There are many potential places to improve this investigation:
- 30 tasks is less than 1/10th of the full Verified dataset. Furthermore, the 30 tasks selected, while randomly chosen, ended up being skewed towards a few repositories. Testing on a larger, more evenly distributed dataset could give us a more robust view of how well the ensemble method works relative to individual agents.
- The generated test cases used from the UTBoost repository were very limited. Generating more robust test cases with a SOTA model could dramatically improve the ensemble selection process.
- We use an arbitrary tiebreaker here, which leads to us picking a suboptimal agent in the case of a tie. If we improved the tiebreaker methods, we could see better performance by the ensemble method. One potential idea is using a "validation" set of tasks to approximate the performance of each agent. For example, select 30 tasks and have each agent individually attempt to resolve the tasks. When selecting agents for new (never seen before) tasks, if there is a tie, break the tie using the agent's performance on the validation set. 

## Technical details

Most of this task was completed through basic scripting. You will find various shell scripts and Python scripts (under the `utils` directory) in the codebase. I am in the process of cleaning up and better documenting the various scripts I used.

I leveraged GitHub Copilot + Clause Sonnet 3.7 agent mode extensively to generate scripts, run commands, and generally make my life easier. 

This task was completed on an EC2 m5.2xlarge instance with a 150 GB gps EBS volume. Running a subset of 30 tasks on all 20 agents took 3-4 hours (this was done twice). It cost about $10 USD in EC2 credits to complete the task (development + testing).

`utils` contains several useful Python scripts that help process and prepare the patches/results. `generated_test_cases.json` contains the generated test cases we used from the UTBoost method. `original_runs` contains the results of the top 20 agents on the original subset of tasks. `new_test_case_runs` contains the results of the top 20 agents on the subset of tasks with our generated test cases injected. Note that the results in `new_test_case_runs` don't accurately reflect whether or not a task was actually resolved due to issues with the SWE-bench parser. `top_models.json` contains information about the best agent per task with our generated test cases. Finally, the `ensemble_model` directory contains the patches we submitted for the ensemble agent as well as the results.

### References

@inproceedings{
    anonymous2025utboost,
    title={{UTB}oost: Rigorous Evaluation of Coding Agents on {SWE}-Bench},
    author={Anonymous},
    booktitle={Submitted to ACL Rolling Review - December 2024},
    year={2025},
    url={https://openreview.net/forum?id=I4tNPWHM6e},
}

@inproceedings{
    jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}

PerplexityAI. (2025). ChatGPT-4.1 [Large Language Model](https://www.perplexity.ai/)

GitHubCopilot. (2025). Claude Sonnet 3.7 [Large Language Model](https://docs.github.com/en/copilot/using-github-copilot/ai-models/using-claude-sonnet-in-github-copilot)