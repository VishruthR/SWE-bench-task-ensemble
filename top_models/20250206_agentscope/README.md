# Agentscope SWE-bench Submission

We achieved a resolve rate of 63.4%(317/500) on the SWE-Bench_Verified Benchmark.


```
Submission summary for 20250206_agentscope on SWE-bench verified split
==================================================
Resolved 317 instances (63.4%)
==================================================
Resolved by Repository
- astropy/astropy: 11/22 (50.0%)
- django/django: 157/231 (67.97%)
- matplotlib/matplotlib: 18/34 (52.94%)
- mwaskom/seaborn: 0/2 (0.0%)
- pallets/flask: 1/1 (100.0%)
- psf/requests: 7/8 (87.5%)
- pydata/xarray: 14/22 (63.64%)
- pylint-dev/pylint: 2/10 (20.0%)
- pytest-dev/pytest: 13/19 (68.42%)
- scikit-learn/scikit-learn: 26/32 (81.25%)
- sphinx-doc/sphinx: 24/44 (54.55%)
- sympy/sympy: 44/75 (58.67%)
==================================================
Resolved by Time
- 2013: 3/3 (100.0%)
- 2014: 2/2 (100.0%)
- 2015: 1/1 (100.0%)
- 2016: 2/2 (100.0%)
- 2017: 12/16 (75.0%)
- 2018: 13/24 (54.17%)
- 2019: 68/98 (69.39%)
- 2020: 71/108 (65.74%)
- 2021: 49/86 (56.98%)
- 2022: 61/102 (59.8%)
- 2023: 35/58 (60.34%)
```


## Our Solution

Here we provide a brief overview of the solution employed. A more comprehensive explanation is available in our [**blog post**](https://doc.agentscope.io/tutorial/swe.html).

### Main Process
Our approach divides each trial into multiple sections, with different agents independently completing specific tasks. We use Claude-3.5-Sonnet-20241022 here.

The main sections include:

#### Reproduction
- Reproduce the bug mentioned in the issue, creating a `reproduction_test.py` file.
- The reproduction serves as a benchmark for verifying the bug fix.

#### Fixing
- Modify the repository code to pass the bug test in `reproduction_test.py`.

#### Testing
- Identify and execute relevant unit tests in the repository.
- Verify that the code changes don't break existing functionality.
- If regressions are detected, the agent will continue to make further modifications.

### Voting Process

We trained a Reward Model to select the best result from our trials.

The Reward Model is based on Qwen-2.5-Coder with an added scalar head, trained using the [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) framework.

## Trajectories
In the `/trajs` directory, we have provided full logs for our four trial runs, along with the message input and output results from the voting process.

## Acknowledgements
We express our gratitude to the SWE-bench team for creating an excellent benchmark, and we thank the all the teams who have openly shared their methods. During our development process, our team learned a great deal from these publicly available approaches. We would like to thank the [Nebius](https://nebius.com/blog/posts/training-and-search-for-software-engineering-agents) and [SWE-Gym](https://github.com/SWE-Gym/SWE-Gym) teams for their open datasets.

## Submission Checklist
- [x] Is a pass@1 submission (does not attempt the same task instance more than once)
- [x] Does not use SWE-bench test knowledge (`PASS_TO_PASS`, `FAIL_TO_PASS`)
- [x] Does not use the `hints` field in SWE-bench
- [x] Does not have web-browsing OR has taken steps to prevent lookup of SWE-bench solutions via web-browsing
